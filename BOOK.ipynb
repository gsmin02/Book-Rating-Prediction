{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3513001d-3ad3-490d-857e-d912a210269a",
   "metadata": {},
   "source": [
    "## 라이브러리 임포트 및 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947dfeb0-9683-402d-97d9-0939093408ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953ae55-38af-445d-a13f-fd0621ac2083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isbn                        0\n",
      "book_title                  0\n",
      "book_author                -1\n",
      "year_of_publication       0.0\n",
      "publisher                   0\n",
      "img_url                     0\n",
      "language               -67227\n",
      "category               -68851\n",
      "summary                -67227\n",
      "img_path                    0\n",
      "dtype: object\n",
      "user_id         0.0\n",
      "location          0\n",
      "age        -27833.0\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_book = pd.read_csv(\"data/books.csv\")\n",
    "df_user = pd.read_csv(\"data/users.csv\")\n",
    "df_train = pd.read_csv(\"data/train_ratings.csv\")\n",
    "df_test = pd.read_csv(\"data/test_ratings.csv\")\n",
    "\n",
    "book_na = df_book.isna()\n",
    "user_na = df_user.isna()\n",
    "\n",
    "df_book = df_book.fillna(-1)\n",
    "df_user = df_user.fillna(-1)\n",
    "\n",
    "print(df_book[book_na].sum())\n",
    "print(df_user[user_na].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4659964a-ac97-458a-a6cc-0625ef964feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'location', 'age'], dtype='object')\n",
      "Index(['isbn', 'book_title', 'book_author', 'year_of_publication', 'publisher',\n",
      "       'img_url', 'language', 'category', 'summary', 'img_path'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_user.columns)\n",
    "print(df_book.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e8c7a-4d81-4d86-b7b1-39f0fb61df64",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    "* 고유 번호 생성 및 매핑\n",
    "* 통계량 추출\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c5dabb-a895-4914-914b-6f8e0fbd61ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  location  age\n",
      "0        0         0    0\n",
      "1        1         1    1\n",
      "2        2         2    0\n",
      "   isbn                                         book_title  book_author  \\\n",
      "0     0                                       Clara Callan            0   \n",
      "1     1                               Decision in Normandy            1   \n",
      "2     2  Flu: The Story of the Great Influenza Pandemic...            2   \n",
      "\n",
      "   year_of_publication  publisher  \\\n",
      "0                    0          0   \n",
      "1                    1          1   \n",
      "2                    2          2   \n",
      "\n",
      "                                             img_url  language  category  \\\n",
      "0  http://images.amazon.com/images/P/0002005018.0...         0         0   \n",
      "1  http://images.amazon.com/images/P/0060973129.0...         0         1   \n",
      "2  http://images.amazon.com/images/P/0374157065.0...         0         2   \n",
      "\n",
      "                                             summary  \\\n",
      "0  In a small town in Canada, Clara Callan reluct...   \n",
      "1  Here, for the first time in paperback, is an o...   \n",
      "2  Describes the great flu epidemic of 1918, an o...   \n",
      "\n",
      "                            img_path  \n",
      "0  images/0002005018.01.THUMBZZZ.jpg  \n",
      "1  images/0060973129.01.THUMBZZZ.jpg  \n",
      "2  images/0374157065.01.THUMBZZZ.jpg  \n"
     ]
    }
   ],
   "source": [
    "isbn2id = {}\n",
    "id2isbn = []\n",
    "for new_id, old_id in enumerate(df_book['isbn'].unique()):\n",
    "    isbn2id[old_id] = new_id\n",
    "    id2isbn.append(old_id)\n",
    "\n",
    "df_book['isbn'] = df_book['isbn'].map(isbn2id)\n",
    "df_train['isbn'] = df_train['isbn'].map(isbn2id)\n",
    "df_test['isbn'] = df_test['isbn'].map(isbn2id)\n",
    "\n",
    "user2id = {}\n",
    "id2user = []\n",
    "for new_id, old_id in enumerate(df_user['user_id'].unique()):\n",
    "    user2id[old_id] = new_id\n",
    "    id2isbn.append(old_id)\n",
    "\n",
    "df_user['user_id'] = df_user['user_id'].map(user2id)\n",
    "df_train['user_id'] = df_train['user_id'].map(user2id)\n",
    "df_test['user_id'] = df_test['user_id'].map(user2id)\n",
    "\n",
    "location2id = {}\n",
    "id2location = []\n",
    "for new_id, old_id in enumerate(df_user['location'].unique()):\n",
    "    location2id[old_id] = new_id\n",
    "    id2location.append(old_id)\n",
    "\n",
    "df_user['location'] = df_user['location'].map(location2id)\n",
    "\n",
    "age2id = {}\n",
    "id2age = []\n",
    "for new_id, old_id in enumerate(df_user['age'].unique()):\n",
    "    age2id[old_id] = new_id\n",
    "    id2age.append(old_id)\n",
    "\n",
    "df_user['age'] = df_user['age'].map(age2id)\n",
    "\n",
    "author2id = {}\n",
    "id2author = []\n",
    "for new_id, old_id in enumerate(df_book['book_author'].unique()):\n",
    "    author2id[old_id] = new_id\n",
    "    id2author.append(old_id)\n",
    "\n",
    "df_book['book_author'] = df_book['book_author'].map(author2id)\n",
    "\n",
    "year2id = {}\n",
    "id2year = []\n",
    "for new_id, old_id in enumerate(df_book['year_of_publication'].unique()):\n",
    "    year2id[old_id] = new_id\n",
    "    id2year.append(old_id)\n",
    "\n",
    "df_book['year_of_publication'] = df_book['year_of_publication'].map(year2id)\n",
    "\n",
    "publisher2id = {}\n",
    "id2publisher = []\n",
    "for new_id, old_id in enumerate(df_book['publisher'].unique()):\n",
    "    publisher2id[old_id] = new_id\n",
    "    id2publisher.append(old_id)\n",
    "\n",
    "df_book['publisher'] = df_book['publisher'].map(publisher2id)\n",
    "\n",
    "language2id = {}\n",
    "id2language = []\n",
    "for new_id, old_id in enumerate(df_book['language'].unique()):\n",
    "    language2id[old_id] = new_id\n",
    "    id2language.append(old_id)\n",
    "\n",
    "df_book['language'] = df_book['language'].map(language2id)\n",
    "\n",
    "category2id = {}\n",
    "id2category = []\n",
    "for new_id, old_id in enumerate(df_book['category'].unique()):\n",
    "    category2id[old_id] = new_id\n",
    "    id2category.append(old_id)\n",
    "\n",
    "df_book['category'] = df_book['category'].map(category2id)\n",
    "\n",
    "print(df_user.head(3))\n",
    "print(df_book.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf828877-8378-4f5b-aad2-7e8312cedbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_book['img_url']\n",
    "del df_book['summary']\n",
    "del df_book['img_path']\n",
    "del df_book['book_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50aa5b18-10b3-4b3b-ae25-9e857b3d71e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id    isbn  rating  book_author  year_of_publication  publisher  \\\n",
      "0             0       0       4            0                    0          0   \n",
      "1             3       0       7            0                    0          0   \n",
      "2             7       0       8            0                    0          0   \n",
      "3             9       0       8            0                    0          0   \n",
      "4            10       0       9            0                    0          0   \n",
      "...         ...     ...     ...          ...                  ...        ...   \n",
      "306790     4515  149564       7         1323                    5       2340   \n",
      "306791     4744  149565       6           72                   12        228   \n",
      "306792     4744  149567       7        62056                   19      11569   \n",
      "306793     4744  149568       7        62057                   11       7026   \n",
      "306794     4744  149569      10        62058                    2      11570   \n",
      "\n",
      "        language  category  location  age  ...  year_std  publisher_count  \\\n",
      "0              0         0         0    0  ...  2.389595               19   \n",
      "1              0         0         3    2  ...  2.389595               19   \n",
      "2              0         0         6    0  ...  2.389595               19   \n",
      "3              0         0         7    0  ...  2.389595               19   \n",
      "4              0         0         8    0  ...  2.389595               19   \n",
      "...          ...       ...       ...  ...  ...       ...              ...   \n",
      "306790         1         5      2635   45  ...  2.353259               83   \n",
      "306791         0         7        35   28  ...  2.486742              881   \n",
      "306792         1         5        35   28  ...  2.525974                1   \n",
      "306793         0         3        35   28  ...  2.438696                3   \n",
      "306794         1         5        35   28  ...  2.452325                1   \n",
      "\n",
      "        publisher_mean  publisher_std  language_count  language_mean  \\\n",
      "0             6.947368       1.580214          182282       7.089060   \n",
      "1             6.947368       1.580214          182282       7.089060   \n",
      "2             6.947368       1.580214          182282       7.089060   \n",
      "3             6.947368       1.580214          182282       7.089060   \n",
      "4             6.947368       1.580214          182282       7.089060   \n",
      "...                ...            ...             ...            ...   \n",
      "306790        6.156627       2.791459          119084       7.048546   \n",
      "306791        6.918275       2.440686          182282       7.089060   \n",
      "306792        7.000000      -1.000000          119084       7.048546   \n",
      "306793        6.333333       4.041452          182282       7.089060   \n",
      "306794       10.000000      -1.000000          119084       7.048546   \n",
      "\n",
      "        language_std  category_count  category_mean  category_std  \n",
      "0           2.430033              14       6.928571      2.302650  \n",
      "1           2.430033              14       6.928571      2.302650  \n",
      "2           2.430033              14       6.928571      2.302650  \n",
      "3           2.430033              14       6.928571      2.302650  \n",
      "4           2.430033              14       6.928571      2.302650  \n",
      "...              ...             ...            ...           ...  \n",
      "306790      2.435525          121221       7.044769      2.439274  \n",
      "306791      2.430033            3132       7.363985      2.340202  \n",
      "306792      2.435525          121221       7.044769      2.439274  \n",
      "306793      2.430033          108569       6.978585      2.413085  \n",
      "306794      2.435525          121221       7.044769      2.439274  \n",
      "\n",
      "[306795 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "cols = [\n",
    "    'isbn', \n",
    "    'book_author', \n",
    "    'year_of_publication', \n",
    "    'publisher', \n",
    "    'language', \n",
    "    'category'\n",
    "]\n",
    "\n",
    "df = df_train.merge(df_book[cols], on='isbn', how='left')\n",
    "df = df.merge(df_user, on='user_id', how='left')\n",
    "\n",
    "# add counts of user / location / age / book / author / year of publication / publisher / language / category \n",
    "stats_of_user = df.groupby('user_id')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_user.columns = ['user_id', 'user_count', 'user_mean', 'user_std']\n",
    "df = df.merge(stats_of_user, on='user_id', how='left')\n",
    "\n",
    "stats_of_location = df.groupby('location')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_location.columns = ['location', 'location_count', 'location_mean', 'location_std']\n",
    "df = df.merge(stats_of_location, on='location', how='left')\n",
    "\n",
    "mask = df['location'] == -1\n",
    "df.loc[mask, 'location_count'] = 0\n",
    "df.loc[mask, 'location_mean'] = df.loc[mask, 'rating']\n",
    "df.loc[mask, 'location_std'] = 0\n",
    "\n",
    "stats_of_age = df.groupby('age')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_age.columns = ['age', 'age_count', 'age_mean', 'age_std']\n",
    "df = df.merge(stats_of_age, on='age', how='left')\n",
    "\n",
    "mask = df['age'] == -1\n",
    "df.loc[mask, 'age'] = 0\n",
    "df.loc[mask, 'age'] = df.loc[mask, 'rating']\n",
    "df.loc[mask, 'age'] = 0\n",
    "\n",
    "stats_of_book = df.groupby('isbn')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_book.columns = ['isbn', 'book_count', 'book_mean', 'book_std']\n",
    "df = df.merge(stats_of_book, on='isbn', how='left')\n",
    "\n",
    "stats_of_author = df.groupby('book_author')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_author.columns = ['book_author', 'author_count', 'author_mean', 'author_std']\n",
    "df = df.merge(stats_of_author, on='book_author', how='left')\n",
    "\n",
    "mask = df['book_author'] == -1\n",
    "df.loc[mask, 'book_author'] = 0\n",
    "df.loc[mask, 'book_author'] = df.loc[mask, 'rating']\n",
    "df.loc[mask, 'book_author'] = 0\n",
    "\n",
    "stats_of_year = df.groupby('year_of_publication')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_year.columns = ['year_of_publication', 'year_count', 'year_mean', 'year_std']\n",
    "df = df.merge(stats_of_year, on='year_of_publication', how='left')\n",
    "\n",
    "mask = df['year_of_publication'] == -1\n",
    "df.loc[mask, 'year_of_publication'] = 0\n",
    "df.loc[mask, 'year_of_publication'] = df.loc[mask, 'rating']\n",
    "df.loc[mask, 'year_of_publication'] = 0\n",
    "\n",
    "stats_of_publisher = df.groupby('publisher')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_publisher.columns = ['publisher', 'publisher_count', 'publisher_mean', 'publisher_std']\n",
    "df = df.merge(stats_of_publisher, on='publisher', how='left')\n",
    "\n",
    "mask = df['publisher'] == -1\n",
    "df.loc[mask, 'publisher'] = 0\n",
    "df.loc[mask, 'publisher'] = df.loc[mask, 'rating']\n",
    "df.loc[mask, 'publisher'] = 0\n",
    "\n",
    "stats_of_language = df.groupby('language')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_language.columns = ['language', 'language_count', 'language_mean', 'language_std']\n",
    "df = df.merge(stats_of_language, on='language', how='left')\n",
    "\n",
    "mask = df['language'] == -1\n",
    "df.loc[mask, 'language'] = 0\n",
    "df.loc[mask, 'language'] = df.loc[mask, 'rating']\n",
    "df.loc[mask, 'language'] = 0\n",
    "\n",
    "stats_of_category = df.groupby('category')['rating'].agg(['count', 'mean', 'std']).reset_index().fillna(-1)\n",
    "stats_of_category.columns=  ['category', 'category_count', 'category_mean', 'category_std']\n",
    "df = df.merge(stats_of_category, on='category', how='left')\n",
    "\n",
    "mask = df['category'] == -1\n",
    "df.loc[mask, 'category'] = 0\n",
    "df.loc[mask, 'category'] = df.loc[mask, 'rating']\n",
    "df.loc[mask, 'category'] = 0\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a397c9-bf0a-4280-9122-0e136731b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'location', 'age', 'user_count', 'user_mean', 'user_std',\n",
      "       'location_count', 'location_mean', 'location_std', 'age_count',\n",
      "       'age_mean', 'age_std'],\n",
      "      dtype='object')\n",
      "Index(['isbn', 'book_author', 'year_of_publication', 'publisher', 'language',\n",
      "       'category', 'book_count', 'book_mean', 'book_std', 'author_count',\n",
      "       'author_mean', 'author_std', 'year_count', 'year_mean', 'year_std',\n",
      "       'publisher_count', 'publisher_mean', 'publisher_std', 'language_count',\n",
      "       'language_mean', 'language_std', 'category_count', 'category_mean',\n",
      "       'category_std'],\n",
      "      dtype='object')\n",
      "user_id              0\n",
      "location             0\n",
      "age                  0\n",
      "user_count        8289\n",
      "user_mean         8289\n",
      "user_std          8289\n",
      "location_count    8289\n",
      "location_mean     8289\n",
      "location_std      8289\n",
      "age_count         8289\n",
      "age_mean          8289\n",
      "age_std           8289\n",
      "dtype: int64\n",
      "isbn                       0\n",
      "book_author                0\n",
      "year_of_publication        0\n",
      "publisher                  0\n",
      "language                   0\n",
      "category                   0\n",
      "book_count             19793\n",
      "book_mean              19793\n",
      "book_std               19793\n",
      "author_count           19793\n",
      "author_mean            19793\n",
      "author_std             19793\n",
      "year_count             19793\n",
      "year_mean              19793\n",
      "year_std               19793\n",
      "publisher_count        19793\n",
      "publisher_mean         19793\n",
      "publisher_std          19793\n",
      "language_count         19793\n",
      "language_mean          19793\n",
      "language_std           19793\n",
      "category_count         19793\n",
      "category_mean          19793\n",
      "category_std           19793\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "user_cols = ['user_id', 'user_count', 'user_mean',\n",
    "       'user_std', 'location_count', 'location_mean', 'location_std',\n",
    "       'age_count', 'age_mean', 'age_std']\n",
    "df_user = df_user.merge(df[user_cols], on='user_id', how='left')\n",
    "\n",
    "book_cols = ['isbn', 'book_count', 'book_mean',\n",
    "       'book_std', 'author_count', 'author_mean', 'author_std', 'year_count',\n",
    "       'year_mean', 'year_std', 'publisher_count', 'publisher_mean',\n",
    "       'publisher_std', 'language_count', 'language_mean', 'language_std',\n",
    "       'category_count', 'category_mean', 'category_std']\n",
    "df_book = df_book.merge(df[book_cols], on='isbn', how='left')\n",
    "\n",
    "\n",
    "print(df_user.columns)\n",
    "print(df_book.columns)\n",
    "print(df_user.isna().sum())\n",
    "print(df_book.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd0eae9-9022-4db3-bb24-fa57ec8ba04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id           0\n",
      "location          0\n",
      "age               0\n",
      "user_count        0\n",
      "user_mean         0\n",
      "user_std          0\n",
      "location_count    0\n",
      "location_mean     0\n",
      "location_std      0\n",
      "age_count         0\n",
      "age_mean          0\n",
      "age_std           0\n",
      "dtype: int64\n",
      "isbn                   0\n",
      "book_author            0\n",
      "year_of_publication    0\n",
      "publisher              0\n",
      "language               0\n",
      "category               0\n",
      "book_count             0\n",
      "book_mean              0\n",
      "book_std               0\n",
      "author_count           0\n",
      "author_mean            0\n",
      "author_std             0\n",
      "year_count             0\n",
      "year_mean              0\n",
      "year_std               0\n",
      "publisher_count        0\n",
      "publisher_mean         0\n",
      "publisher_std          0\n",
      "language_count         0\n",
      "language_mean          0\n",
      "language_std           0\n",
      "category_count         0\n",
      "category_mean          0\n",
      "category_std           0\n",
      "dtype: int64\n",
      "        user_id  location  age  user_count  user_mean  user_std  \\\n",
      "0             0         0    0         7.0   4.428571  1.988060   \n",
      "7             1         1    1        12.0   6.750000  2.137331   \n",
      "19            2         2    0      5520.0   6.779891  2.608527   \n",
      "5539          3         3    2         7.0   7.285714  0.487950   \n",
      "5546          4         4    3       120.0   7.666667  1.620941   \n",
      "...         ...       ...  ...         ...        ...       ...   \n",
      "315079    68087      6898   40         1.0   7.000000 -1.000000   \n",
      "315080    68088     18366   36         1.0   8.000000 -1.000000   \n",
      "315081    68089     18367    0         1.0   2.000000 -1.000000   \n",
      "315082    68090       200   28         1.0  10.000000 -1.000000   \n",
      "315083    68091       482   66         1.0   7.000000 -1.000000   \n",
      "\n",
      "        location_count  location_mean  location_std  age_count  age_mean  \\\n",
      "0                 15.0       6.266667      2.344192    92662.0  6.735404   \n",
      "7               2261.0       7.178240      2.378291     4688.0  7.553541   \n",
      "19              5525.0       6.781176      2.607863    92662.0  6.735404   \n",
      "5539            4692.0       6.836104      2.374676     7090.0  7.303808   \n",
      "5546             821.0       6.868453      2.349830     6174.0  7.217363   \n",
      "...                ...            ...           ...        ...       ...   \n",
      "315079            16.0       7.812500      1.108678     2790.0  7.426165   \n",
      "315080             1.0       8.000000     -1.000000      136.0  7.411765   \n",
      "315081             1.0       2.000000     -1.000000    92662.0  6.735404   \n",
      "315082           252.0       6.789683      2.195480     8084.0  6.882608   \n",
      "315083           839.0       6.640048      2.700503     1342.0  8.207154   \n",
      "\n",
      "         age_std  \n",
      "0       2.529725  \n",
      "7       2.128703  \n",
      "19      2.529725  \n",
      "5539    2.344188  \n",
      "5546    2.352615  \n",
      "...          ...  \n",
      "315079  2.321296  \n",
      "315080  2.342745  \n",
      "315081  2.529725  \n",
      "315082  2.553149  \n",
      "315083  2.203382  \n",
      "\n",
      "[68092 rows x 12 columns]\n",
      "          isbn  book_author  year_of_publication  publisher  language  \\\n",
      "0            0            0                    0          0         0   \n",
      "7            1            1                    1          1         0   \n",
      "8            2            2                    2          2         0   \n",
      "13           3            3                    1          3         0   \n",
      "27           4            4                    3          4         0   \n",
      "...        ...          ...                  ...        ...       ...   \n",
      "326583  149565           72                   12        228         0   \n",
      "326584  149566        62055                    0        230         0   \n",
      "326585  149567        62056                   19      11569         1   \n",
      "326586  149568        62057                   11       7026         0   \n",
      "326587  149569        62058                    2      11570         1   \n",
      "\n",
      "        category  book_count  book_mean  book_std  author_count  ...  \\\n",
      "0              0         7.0   6.857143  1.772811           7.0  ...   \n",
      "7              1         1.0   8.000000 -1.000000           2.0  ...   \n",
      "8              2         5.0   7.600000  2.073644           5.0  ...   \n",
      "13             3        14.0   7.571429  1.785165         548.0  ...   \n",
      "27             4         1.0   8.000000 -1.000000           7.0  ...   \n",
      "...          ...         ...        ...       ...           ...  ...   \n",
      "326583         7         1.0   6.000000 -1.000000          29.0  ...   \n",
      "326584         6         0.0   0.000000  0.000000           0.0  ...   \n",
      "326585         5         1.0   7.000000 -1.000000           1.0  ...   \n",
      "326586         3         1.0   7.000000 -1.000000           1.0  ...   \n",
      "326587         5         1.0  10.000000 -1.000000           1.0  ...   \n",
      "\n",
      "        year_std  publisher_count  publisher_mean  publisher_std  \\\n",
      "0       2.389595             19.0        6.947368       1.580214   \n",
      "7       2.458491              8.0        7.500000       2.563480   \n",
      "8       2.452325            299.0        6.936455       2.499525   \n",
      "13      2.458491           1137.0        6.855761       2.405676   \n",
      "27      2.454321           7441.0        6.871523       2.381017   \n",
      "...          ...              ...             ...            ...   \n",
      "326583  2.486742            881.0        6.918275       2.440686   \n",
      "326584  0.000000              0.0        0.000000       0.000000   \n",
      "326585  2.525974              1.0        7.000000      -1.000000   \n",
      "326586  2.438696              3.0        6.333333       4.041452   \n",
      "326587  2.452325              1.0       10.000000      -1.000000   \n",
      "\n",
      "        language_count  language_mean  language_std  category_count  \\\n",
      "0             182282.0       7.089060      2.430033            14.0   \n",
      "7             182282.0       7.089060      2.430033             1.0   \n",
      "8             182282.0       7.089060      2.430033           333.0   \n",
      "13            182282.0       7.089060      2.430033        108569.0   \n",
      "27            182282.0       7.089060      2.430033          2672.0   \n",
      "...                ...            ...           ...             ...   \n",
      "326583        182282.0       7.089060      2.430033          3132.0   \n",
      "326584             0.0       0.000000      0.000000             0.0   \n",
      "326585        119084.0       7.048546      2.435525        121221.0   \n",
      "326586        182282.0       7.089060      2.430033        108569.0   \n",
      "326587        119084.0       7.048546      2.435525        121221.0   \n",
      "\n",
      "        category_mean  category_std  \n",
      "0            6.928571      2.302650  \n",
      "7            8.000000     -1.000000  \n",
      "8            7.312312      2.290315  \n",
      "13           6.978585      2.413085  \n",
      "27           7.297904      2.376005  \n",
      "...               ...           ...  \n",
      "326583       7.363985      2.340202  \n",
      "326584       0.000000      0.000000  \n",
      "326585       7.044769      2.439274  \n",
      "326586       6.978585      2.413085  \n",
      "326587       7.044769      2.439274  \n",
      "\n",
      "[149570 rows x 24 columns]\n",
      "12\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "df_user = df_user.drop_duplicates().fillna(0)\n",
    "df_book = df_book.drop_duplicates().fillna(0)\n",
    "\n",
    "print(df_user.isna().sum())\n",
    "print(df_book.isna().sum())\n",
    "print(df_user)\n",
    "print(df_book)\n",
    "print(len(df_user.columns))\n",
    "print(len(df_book.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6892f654-5129-42bf-9cbf-98bb3499a38b",
   "metadata": {},
   "source": [
    "## 학습 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea09a270-d78b-4e87-8c5a-2cee16e8f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bookset(Dataset):\n",
    "    def __init__(self, df_train, mode='train', device='mps'):\n",
    "\"\"\"\n",
    "        PyTorch Dataset: 추천 시스템 모델 학습을 위한 데이터셋 클래스입니다.\n",
    "        상호작용에 따른 유저, 아이템, 특징들의 ID를 분할하고 torch.Tensor 형태로 변환하여 관리합니다.\n",
    "\n",
    "        입력 데이터프레임은 모든 특징 컬럼이 [0, size-1] 범위로 인코딩된 상태(전처리가 완료된 상태)여야 합니다.\n",
    "\n",
    "        (파라미터 Parameters)\n",
    "        :param df_train: 모든 상호작용 정보와 인코딩된 특징 ID를 포함하는 데이터프레임입니다.\n",
    "        :type df_train: pandas.DataFrame\n",
    "        :param mode: 데이터 분할 모드를 결정합니다. 'train'일 경우 80%, 'test'일 경우 20%를 사용합니다.\n",
    "        :type mode: str\n",
    "        :param device: 데이터를 로드할 장치 (CPU, CUDA, MPS 등).\n",
    "        :type device: str\n",
    "\n",
    "        (클래스 속성 Attributes: 모두 torch.Tensor, dtype=torch.long)\n",
    "        :attr user: 유저 고유 ID (user_id)\n",
    "        :attr user_f0: 지역 고유 ID (location)\n",
    "        :attr user_f1: 나이 고유 ID (age)\n",
    "        :attr item: 책 고유 ID (isbn)\n",
    "        :attr item_f0: 저자 고유 ID (book_author)\n",
    "        :attr item_f1: 연도 고유 ID (year_of_publication)\n",
    "        :attr item_f2: 출판사 고유 ID (publisher)\n",
    "        :attr item_f3: 언어 고유 ID (language)\n",
    "        :attr item_f4: 카테고리 고유 ID (category)\n",
    "        :attr y: 정답 레이블 (rating). (dtype=torch.float)\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        train_cols = ['user_id', 'isbn', 'location', 'age',\n",
    "                      'book_author', 'year_of_publication',\n",
    "                      'publisher', 'language', 'category']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df_train[train_cols],\n",
    "            df_train['rating'],\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        if mode == 'train':\n",
    "            X = X_train\n",
    "            y = y_train\n",
    "        else:\n",
    "            X = X_test\n",
    "            y = y_test\n",
    "\n",
    "        self.user = torch.tensor(X['user_id'].values, dtype=torch.long, device=device)\n",
    "        self.user_f0 = torch.tensor(X['location'].values, dtype=torch.long, device=device)\n",
    "        self.user_f1 = torch.tensor(X['age'].values, dtype=torch.long, device=device)\n",
    "\n",
    "        self.item = torch.tensor(X['isbn'].values, dtype=torch.long, device=device)\n",
    "        self.item_f0 = torch.tensor(X['book_author'].values, dtype=torch.long, device=device)\n",
    "        self.item_f1 = torch.tensor(X['year_of_publication'].values, dtype=torch.long, device=device)\n",
    "        self.item_f2 = torch.tensor(X['publisher'].values, dtype=torch.long, device=device)\n",
    "        self.item_f3 = torch.tensor(X['language'].values, dtype=torch.long, device=device)\n",
    "        self.item_f4 = torch.tensor(X['category'].values, dtype=torch.long, device=device)\n",
    "\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋에 포함된 총 상호작용 개수를 반환합니다.\n",
    "        \n",
    "        :returns: 상호작용 개수\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        \n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        특정 인덱스에 해당하는 상호작용의 특징들과 레이블을 반환합니다.\n",
    "\n",
    "        :param idx: 데이터 인덱스\n",
    "        :type idx: int\n",
    "        :returns: (특징 튜플, 레이블 텐서)\n",
    "        :rtype: tuple of (tuple of torch.Tensor, torch.Tensor)\n",
    "        \"\"\"\n",
    "        \n",
    "        return (\n",
    "            self.user[idx],\n",
    "            self.item[idx],\n",
    "            self.user_f0[idx],\n",
    "            self.user_f1[idx],\n",
    "            self.item_f0[idx],\n",
    "            self.item_f1[idx],\n",
    "            self.item_f2[idx],\n",
    "            self.item_f3[idx],\n",
    "            self.item_f4[idx]\n",
    "        ), self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c82f9-bcb3-4aae-86dd-073062af36b5",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57acc2c3-4b04-4842-8f60-23548b92e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    ID, 특징 ID 임베딩과 CLIP 이미지, 텍스트 임베딩을 통합해서 별점을 예측하는 모델입니다.\n",
    "    \n",
    "    특징 가중치 (self.w)와 아이템 빈도(freq) 기반의 가중치 메커니즘을 사용합니다.\n",
    "\n",
    "    (파라미터 Parameters)\n",
    "    :param user_cnt: 유저 ID의 총 개수 (임베딩 테이블 크기).\n",
    "    :param item_cnt: 아이템 ID의 총 개수 (임베딩 테이블 크기).\n",
    "    :param user_f_cnt: 유저 특징별 개수 (list of int, [location_cnt, age_cnt]).\n",
    "    :param item_f_cnt: 아이템 특징별 개수 (list of int, [author_cnt, year_cnt, ..., category_cnt]).\n",
    "    :param embedding_dim: 임베딩 벡터의 차원 수 (d).\n",
    "    :param freq_of_item: 각 아이템의 통계량 (빈도, 평균, 표준편차)을 포함하는 numpy 배열.\n",
    "    :param device: 모델이 올라갈 장치 (CPU, CUDA, MPS 등).\n",
    "\n",
    "    (모듈 Modules)\n",
    "    :attr user_emb, item_emb, item_fX_emb: 각 ID 및 특징별 임베딩 테이블.\n",
    "    :attr img_proj, txt_proj: CLIP 512차원 임베딩을 모델의 embedding_dim으로 투영하는 MLP.\n",
    "    :attr mlp: 최종 예측을 수행하는 Multi-Layer Perceptron.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, user_cnt, item_cnt, user_f_cnt, item_f_cnt, embedding_dim, freq_of_item, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # 임베딩 정의\n",
    "        self.user_emb = nn.Embedding(user_cnt, embedding_dim)\n",
    "        self.user_f0_emb = nn.Embedding(user_f_cnt[0], embedding_dim)\n",
    "        self.user_f1_emb = nn.Embedding(user_f_cnt[1], embedding_dim)\n",
    "\n",
    "        self.item_emb = nn.Embedding(item_cnt, embedding_dim)\n",
    "        self.item_f0_emb = nn.Embedding(item_f_cnt[0], embedding_dim)\n",
    "        self.item_f1_emb = nn.Embedding(item_f_cnt[1], embedding_dim)\n",
    "        self.item_f2_emb = nn.Embedding(item_f_cnt[2], embedding_dim)\n",
    "        self.item_f3_emb = nn.Embedding(item_f_cnt[3], embedding_dim)\n",
    "        self.item_f4_emb = nn.Embedding(item_f_cnt[4], embedding_dim)\n",
    "\n",
    "        # 초기화\n",
    "        for emb in [self.user_emb, self.item_emb, self.user_f0_emb, self.user_f1_emb, self.item_f0_emb, self.item_f1_emb, self.item_f2_emb, self.item_f3_emb, self.item_f4_emb]:\n",
    "            nn.init.normal_(emb.weight, mean=0, std=0.01)\n",
    "\n",
    "        # CLIP 임베딩 불러오기\n",
    "        img_emb = torch.from_numpy(np.load(\"data/img_emb_clip.npy\")).float()\n",
    "        txt_emb = torch.from_numpy(np.load(\"data/text_emb_clip.npy\")).float()\n",
    "\n",
    "        self.register_buffer(\"img_emb\", img_emb)\n",
    "        self.register_buffer(\"txt_emb\", txt_emb)\n",
    "\n",
    "        self.img_proj = nn.Sequential(\n",
    "            nn.Linear(512, embedding_dim),\n",
    "            nn.Dropout(0.95),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.txt_proj = nn.Sequential(\n",
    "            nn.Linear(512, embedding_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 통계량 (빈도, 평균, 표준편차)\n",
    "        item_freq = torch.tensor(freq_of_item).float().reshape(-1, 3)\n",
    "        self.freq_raw = nn.Parameter(torch.log(item_freq[:, 0] + 0.00001), requires_grad=False)\n",
    "        self.mean_std = nn.Parameter(item_freq[:, 1:], requires_grad=False)\n",
    "\n",
    "        # 9개 임베딩 가중치 (유저 3 + 아이템 6)\n",
    "        self.w = nn.Parameter(torch.zeros(9))  # 학습 가능한 가중치\n",
    "\n",
    "        # 통합 MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 3 + 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, uf0, uf1, if0, if1, if2, if3, if4):\n",
    "        \n",
    "        # 유저 임베딩\n",
    "        u0 = self.user_emb(user_ids)\n",
    "        u1 = self.user_f0_emb(uf0)\n",
    "        u2 = self.user_f1_emb(uf1)\n",
    "        user_feats = torch.stack([u0, u1, u2], dim=1)\n",
    "\n",
    "        # 아이템 임베딩\n",
    "        i0 = self.item_emb(item_ids)\n",
    "        i1 = self.item_f0_emb(if0)\n",
    "        i2 = self.item_f1_emb(if1)\n",
    "        i3 = self.item_f2_emb(if2)\n",
    "        i4 = self.item_f3_emb(if3)\n",
    "        i5 = self.item_f4_emb(if4)\n",
    "        item_feats = torch.stack([i0, i1, i2, i3, i4, i5], dim=1)\n",
    "        \n",
    "        # 유저 임베딩, 아이템 임베딩 concat\n",
    "        feats = torch.cat([user_feats, item_feats], dim=1)\n",
    "        feats = feats + torch.randn_like(feats).to(device) * 0.1\n",
    "\n",
    "        # 빈도 기반 가중치 생성 (로그빈도 -> 가중치 element-wise 곱 -> 시그모이드)\n",
    "        freq = self.freq_raw[item_ids].unsqueeze(1).repeat(1, 9)\n",
    "        w = self.w.unsqueeze(0)\n",
    "        alpha = torch.sigmoid(freq * w).unsqueeze(-1)\n",
    "\n",
    "        # 빈도 기반 가중치와 concat된 임베딩 벡터 곱\n",
    "        weighted_emb = torch.sum(alpha * feats, dim=1)\n",
    "\n",
    "        # 통계량 concat\n",
    "        ms = self.mean_std[item_ids]\n",
    "        weighted_emb = torch.cat([weighted_emb, ms], dim=1)\n",
    "\n",
    "        # CLIP 임베딩 변환\n",
    "        img = self.img_emb[item_ids]\n",
    "        txt = self.txt_emb[item_ids]\n",
    "\n",
    "        img = self.img_proj(img) * (1 - w.max())\n",
    "        txt = self.txt_proj(txt) * (1 - w.max())\n",
    "\n",
    "        # 유저, 아이템, 특징, 통량 임베딩과 CLIP 임베딩 concat\n",
    "        final_embedding = torch.cat([fused_emb, img, txt], dim=1)\n",
    "\n",
    "        # 최종 레이어\n",
    "        out = self.mlp(final_vec).squeeze(1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3a7e4b-3a6b-4a37-b122-541ea1ffbda2",
   "metadata": {},
   "source": [
    "## 손실함수 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2fa8eaa-28c9-4590-bb61-d2afb2425dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    RMSE 손실함수 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return torch.sqrt(self.mse(pred, target) + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2896ef-ace6-4088-b09b-86312bfce353",
   "metadata": {},
   "source": [
    "## 모델 인자로 넣을 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a1551ac-6f2e-48a3-8061-66656f100afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_f_cnt = [len(user2id), len(location2id), len(age2id)]\n",
    "item_f_cnt = [len(isbn2id), len(author2id), len(year2id), len(publisher2id), len(language2id), len(category2id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf05e4c3-abc5-4cc4-ad30-33bcf09da4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xt/m9lm5qqd4rn3hgsxssh9421r0000gn/T/ipykernel_8476/4120810366.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item_stats = torch.tensor(freq_of_item).float().reshape(-1, 6, 3)\n",
      "/var/folders/xt/m9lm5qqd4rn3hgsxssh9421r0000gn/T/ipykernel_8476/4120810366.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  user_stats = torch.tensor(freq_of_user).float().reshape(-1, 3, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (user_emb): Embedding(68092, 63)\n",
       "  (user_f0_emb): Embedding(68092, 63)\n",
       "  (user_f1_emb): Embedding(18368, 63)\n",
       "  (item_emb): Embedding(149570, 63)\n",
       "  (item_f0_emb): Embedding(149570, 63)\n",
       "  (item_f1_emb): Embedding(62059, 63)\n",
       "  (item_f2_emb): Embedding(95, 63)\n",
       "  (item_f3_emb): Embedding(11571, 63)\n",
       "  (item_f4_emb): Embedding(27, 63)\n",
       "  (img_proj): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (1): Dropout(p=0.95, inplace=False)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (txt_proj): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (1): Dropout(p=0.3, inplace=False)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=704, out_features=320, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=320, out_features=256, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "freq_of_user_np = df_user.values[:, 3:].copy()\n",
    "user_low_freq_mask = (freq_of_user_np[..., 0] == 1) | (freq_of_user_np[..., 0] == 2)\n",
    "observed_mean_user = freq_of_user_np[..., 1][user_low_freq_mask]\n",
    "smoothed_mean_user = (7.06 + observed_mean_user) / 2.0\n",
    "freq_of_user_np[..., 1][user_low_freq_mask] = smoothed_mean_user\n",
    "freq_of_book_np = df_book.values[:, 6:].copy()\n",
    "item_low_freq_mask = (freq_of_book_np[..., 0] == 1) | (freq_of_book_np[..., 0] == 2)\n",
    "observed_mean_item = freq_of_book_np[..., 1][item_low_freq_mask]\n",
    "smoothed_mean_item = (7.06 + observed_mean_item) / 2.0\n",
    "freq_of_book_np[..., 1][item_low_freq_mask] = smoothed_mean_item\n",
    "\n",
    "freq_of_user = torch.from_numpy(freq_of_user_np).to(torch.float).to(device)\n",
    "freq_of_item = torch.from_numpy(freq_of_book_np).to(torch.float).to(device)\n",
    "\n",
    "model = Model(len(df_user), len(df_book), user_f_cnt, item_f_cnt, 64, freq_of_user, freq_of_item, device)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e3fa9a-27c0-4625-8a1c-2ff8b34810c9",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 정의, 옵티마이저 및 손실함수 할당, 데이터 로더 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07c7ddb7-02e8-400e-a595-2cdba59c47c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "lr = 0.035\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005)\n",
    "criterion = RMSELoss()\n",
    "\n",
    "train_set = bookset(df, mode='train', device=device)\n",
    "train_loader = DataLoader(train_set, batch_size=2048, shuffle=True)\n",
    "test_set = bookset(df, mode='test', device=device)\n",
    "test_loader = DataLoader(test_set, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcfd09-46f2-4263-b8f4-08e0271522bb",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa6bd4c9-8d96-41f8-8d85-5d1cf801785e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train=1.6694 test=1.5785\n",
      "Epoch 2 train=1.6434 test=1.6562\n",
      "Epoch 3 train=1.6281 test=1.5697\n",
      "Epoch 4 train=1.6084 test=1.6063\n",
      "Epoch 5 train=1.6344 test=1.5740\n",
      "Epoch 6 train=1.6184 test=1.5737\n",
      "Epoch 7 train=1.6086 test=1.5673\n",
      "Epoch 8 train=1.5981 test=1.5731\n",
      "Epoch 9 train=1.6298 test=1.5815\n",
      "Epoch 10 train=1.6148 test=1.6572\n",
      "Epoch 11 train=1.6100 test=1.5864\n",
      "Epoch 12 train=1.6133 test=1.6090\n",
      "Epoch 13 train=1.5916 test=1.5701\n",
      "Epoch 14 train=1.6129 test=1.6324\n",
      "Epoch 15 train=1.6032 test=1.5956\n",
      "Epoch 16 train=1.5875 test=1.6037\n",
      "Epoch 17 train=1.6224 test=1.5800\n",
      "Epoch 18 train=1.7903 test=1.6441\n",
      "Epoch 19 train=1.6343 test=1.6125\n",
      "Epoch 20 train=1.6065 test=1.6133\n",
      "Epoch 21 train=1.6197 test=1.5941\n",
      "Epoch 22 train=1.6035 test=1.5769\n",
      "Epoch 23 train=1.5974 test=1.6455\n",
      "Epoch 24 train=1.6170 test=1.5889\n",
      "Epoch 25 train=1.6032 test=1.5738\n",
      "Epoch 26 train=1.6022 test=1.7595\n",
      "Epoch 27 train=1.6122 test=1.6573\n",
      "Epoch 28 train=1.6075 test=1.5980\n",
      "Epoch 29 train=1.5985 test=1.5956\n",
      "Epoch 30 train=1.5993 test=1.6009\n",
      "Epoch 31 train=1.6086 test=1.5751\n",
      "Epoch 32 train=1.6289 test=1.5978\n",
      "Epoch 33 train=1.6088 test=1.6083\n",
      "Epoch 34 train=1.5983 test=1.5786\n",
      "Epoch 35 train=1.7460 test=1.6018\n",
      "Epoch 36 train=1.6701 test=1.6504\n",
      "Epoch 37 train=1.6841 test=1.6558\n",
      "Epoch 38 train=1.6720 test=1.5957\n",
      "Epoch 39 train=1.6530 test=1.5955\n",
      "Epoch 40 train=1.8413 test=1.6315\n",
      "Epoch 41 train=1.7323 test=1.6432\n",
      "Epoch 42 train=1.6803 test=1.6260\n",
      "Epoch 43 train=1.6787 test=1.6578\n",
      "Epoch 44 train=1.6733 test=1.5955\n",
      "Epoch 45 train=1.7043 test=1.7285\n",
      "Epoch 46 train=1.6794 test=1.6278\n",
      "Epoch 47 train=1.6661 test=1.5906\n",
      "Epoch 48 train=1.6583 test=1.6756\n",
      "Epoch 49 train=1.6773 test=1.6172\n",
      "Epoch 50 train=1.6670 test=1.5793\n"
     ]
    }
   ],
   "source": [
    "epoch_train_loss = []\n",
    "epoch_test_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_train_loss = 0.0\n",
    "    running_test_loss = 0.0\n",
    "\n",
    "    # ---------- TRAIN ----------\n",
    "    model.train()\n",
    "    n_data = 0\n",
    "\n",
    "    for batch, r in train_loader:\n",
    "        u, i, uf0, uf1, if0, if1, if2, if3, if4 = batch\n",
    "\n",
    "        # device 이동\n",
    "        u = u.to(device)\n",
    "        i = i.to(device)\n",
    "        uf0 = uf0.to(device)\n",
    "        uf1 = uf1.to(device)\n",
    "        if0 = if0.to(device)\n",
    "        if1 = if1.to(device)\n",
    "        if2 = if2.to(device)\n",
    "        if3 = if3.to(device)\n",
    "        if4 = if4.to(device)\n",
    "        r = r.to(device) + (torch.randn(len(r)) * 0.15).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # model forward (feature 포함)\n",
    "        pred = model(u, i, uf0, uf1, if0, if1, if2, if3, if4)\n",
    "\n",
    "        loss = criterion(pred, r)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * len(r)\n",
    "        n_data += len(r)\n",
    "\n",
    "    running_train_loss /= n_data\n",
    "\n",
    "\n",
    "    # ---------- TEST ----------\n",
    "    model.eval()\n",
    "    n_data = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, r in test_loader:\n",
    "            u, i, uf0, uf1, if0, if1, if2, if3, if4 = batch\n",
    "\n",
    "            u = u.to(device)\n",
    "            i = i.to(device)\n",
    "            uf0 = uf0.to(device)\n",
    "            uf1 = uf1.to(device)\n",
    "            if0 = if0.to(device)\n",
    "            if1 = if1.to(device)\n",
    "            if2 = if2.to(device)\n",
    "            if3 = if3.to(device)\n",
    "            if4 = if4.to(device)\n",
    "            r = r.to(device)\n",
    "\n",
    "            pred = model(u, i, uf0, uf1, if0, if1, if2, if3, if4)\n",
    "            loss = criterion(pred, r)\n",
    "\n",
    "            running_test_loss += loss.item() * len(r)\n",
    "            n_data += len(r)\n",
    "\n",
    "    running_test_loss /= n_data\n",
    "\n",
    "    epoch_train_loss.append(running_train_loss)\n",
    "    epoch_test_loss.append(running_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} train={running_train_loss:.4f} test={running_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e39f8-4d73-4aa8-9fdd-d30f9b7af0af",
   "metadata": {},
   "source": [
    "## 최종 제출 파일 생성 코드\n",
    "* 최종 제출을 위한 데이터셋 정의\n",
    "* 추론 및 csv 생성 후 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8b3c9-6540-45da-b226-8186fff11839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class valset(Dataset):\n",
    "    def __init__(self, df_train, device='mps'):\n",
    "        \n",
    "        # ----------- USER -----------\n",
    "        self.user = torch.tensor(df_train['user_id'].values, dtype=torch.long, device=device)\n",
    "        self.user_f0 = torch.tensor(df_train['location'].values, dtype=torch.long, device=device)\n",
    "        self.user_f1 = torch.tensor(df_train['age'].values, dtype=torch.long, device=device)\n",
    "\n",
    "        # ----------- ITEM -----------\n",
    "        self.item = torch.tensor(df_train['isbn'].values, dtype=torch.long, device=device)\n",
    "        self.item_f0 = torch.tensor(df_train['book_author'].values, dtype=torch.long, device=device)\n",
    "        self.item_f1 = torch.tensor(df_train['year_of_publication'].values, dtype=torch.long, device=device)\n",
    "        self.item_f2 = torch.tensor(df_train['publisher'].values, dtype=torch.long, device=device)\n",
    "        self.item_f3 = torch.tensor(df_train['language'].values, dtype=torch.long, device=device)\n",
    "        self.item_f4 = torch.tensor(df_train['category'].values, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user[idx],\\\n",
    "            self.item[idx],\\\n",
    "            self.user_f0[idx],\\\n",
    "            self.user_f1[idx],\\\n",
    "            self.item_f0[idx],\\\n",
    "            self.item_f1[idx],\\\n",
    "            self.item_f2[idx],\\\n",
    "            self.item_f3[idx],\\\n",
    "            self.item_f4[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0decf2e4-7046-4cf9-b1e5-109e85562c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"data/test_ratings.csv\")\n",
    "df_test['user_id'] = df_test['user_id'].map(user2id)\n",
    "df_test['isbn'] = df_test['isbn'].map(isbn2id)\n",
    "\n",
    "user_cols = ['user_id', 'location', 'age']\n",
    "\n",
    "book_cols = ['isbn', 'book_author', 'year_of_publication', 'publisher', 'language',\n",
    "             'category']\n",
    "\n",
    "df_test = df_test.merge(df_book[book_cols], on='isbn', how='left')\n",
    "df_test = df_test.merge(df_user[user_cols], on='user_id', how='left')\n",
    "\n",
    "val_set = valset(df_test, device=device)\n",
    "test_loader = DataLoader(val_set, batch_size=2048, shuffle=False, drop_last=False)\n",
    "\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        u, i, uf0, uf1, if0, if1, if2, if3, if4 = batch\n",
    "\n",
    "        u = u.to(device).long()\n",
    "        i = i.to(device).long()\n",
    "        uf0 = uf0.to(device).long()\n",
    "        uf1 = uf1.to(device).long()\n",
    "        if0 = if0.to(device).long()\n",
    "        if1 = if1.to(device).long()\n",
    "        if2 = if2.to(device).long()\n",
    "        if3 = if3.to(device).long()\n",
    "        if4 = if4.to(device).long()\n",
    "\n",
    "        output = model(u, i, uf0, uf1, if0, if1, if2, if3, if4)\n",
    "        predictions.append(output.cpu().numpy())\n",
    "\n",
    "final_predictions = np.concatenate([o.flatten() for o in predictions])\n",
    "df_submission = pd.read_csv(\"data/test_ratings.csv\")\n",
    "df_submission['rating'] = final_predictions\n",
    "df_submission.to_csv(\"submission.csv\")\n",
    "\n",
    "# 제출 파일 생성 끝"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec310",
   "language": "python",
   "name": "rec310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
